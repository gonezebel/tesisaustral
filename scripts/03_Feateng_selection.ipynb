{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from joblib import Parallel, delayed\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando MaxAbs\n",
      "Memoria inicial: 4.32 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 22/22\n",
      "[SELECTOR] Linear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Linear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Linear_final.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Nonlinear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Nonlinear_final.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Model-Based_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MaxAbs\\MaxAbs_Model-Based_final.parquet\n",
      "\n",
      "Procesando MinMax\n",
      "Memoria inicial: 3.96 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 22/22\n",
      "[SELECTOR] Linear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Linear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Linear_final.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Nonlinear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Nonlinear_final.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Model-Based_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\MinMax\\MinMax_Model-Based_final.parquet\n",
      "\n",
      "Procesando NoNorm\n",
      "Memoria inicial: 4.56 GB\n",
      "Columnas válidas: 90/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 18/18\n",
      "[SELECTOR] Linear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Linear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Linear_final.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Nonlinear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Nonlinear_final.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Model-Based_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\NoNorm\\NoNorm_Model-Based_final.parquet\n",
      "\n",
      "Procesando Robust\n",
      "Memoria inicial: 4.57 GB\n",
      "Columnas válidas: 90/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 18/18\n",
      "[SELECTOR] Linear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Linear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Linear_final.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Nonlinear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Nonlinear_final.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Model-Based_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Robust\\Robust_Model-Based_final.parquet\n",
      "\n",
      "Procesando Standard\n",
      "Memoria inicial: 4.32 GB\n",
      "Columnas válidas: 112/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 23/23\n",
      "[SELECTOR] Linear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Linear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Linear_final.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Nonlinear_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Nonlinear_final.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Reporte CSV guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Model-Based_report.csv\n",
      "✅ Dataset Parquet guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_232410\\Standard\\Standard_Model-Based_final.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================\n",
    "# Configuración de rendimiento y memoria\n",
    "# =============================================\n",
    "MAX_FEATURES = 500\n",
    "COLS_PER_GROUP = 5\n",
    "DTYPE = np.float32\n",
    "SAMPLE_SIZE_MODEL = 50000\n",
    "SAMPLE_FRACTION = 0.3\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Obtiene el uso de memoria actual en GB.\"\"\"\n",
    "    import psutil\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "\n",
    "def is_boolean_column(series):\n",
    "    \"\"\"Verifica si una columna es booleana (0/1).\"\"\"\n",
    "    unique_values = series.dropna().unique()\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "def safe_transform(data, transform_fn):\n",
    "    \"\"\"Aplica transformaciones numéricas seguras.\"\"\"\n",
    "    with np.errstate(all='ignore'):\n",
    "        result = transform_fn(np.abs(data))\n",
    "    return np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0).astype(DTYPE)\n",
    "\n",
    "def process_feature_group(df, cols_group, target_col):\n",
    "    \"\"\"Procesa características incluyendo cuadradas y cúbicas.\"\"\"\n",
    "    features = []\n",
    "    for col in cols_group:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "            \n",
    "        col_data = df[col].astype(DTYPE).values\n",
    "        \n",
    "        if not is_boolean_column(df[col]):\n",
    "            target_data = df[target_col].astype(DTYPE).values\n",
    "            \n",
    "            # Todas las transformaciones\n",
    "            features.extend([\n",
    "                col_data,                                   # Original\n",
    "                safe_transform(col_data, np.log1p),         # Log\n",
    "                safe_transform(col_data, np.sqrt),          # Raíz cuadrada\n",
    "                col_data ** 2,                              # Cuadrada\n",
    "                col_data ** 3,                              # Cúbica\n",
    "                col_data * target_data                       # Interacción\n",
    "            ])\n",
    "        else:\n",
    "            features.append(col_data)\n",
    "    \n",
    "    return np.column_stack(features)\n",
    "\n",
    "def clean_nan(df):\n",
    "    \"\"\"Limpieza agresiva de NaN.\"\"\"\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median(skipna=True)\n",
    "            df[col] = df[col].fillna(median_val if not np.isnan(median_val) else 0.0)\n",
    "    return df\n",
    "\n",
    "def process_dataset(dataset_path, norm_name, experiment_folder):\n",
    "    print(f\"\\nProcesando {norm_name}\")\n",
    "    print(f\"Memoria inicial: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Configuración de directorios\n",
    "    output_folder = os.path.join(experiment_folder, norm_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Carga de datos\n",
    "    ddf = dd.read_parquet(dataset_path)\n",
    "    target_col = 'nivel_triage'\n",
    "    \n",
    "    # Identificar columnas numéricas (excluyendo target)\n",
    "    numeric_cols = [col for col in ddf.columns \n",
    "                   if (col != target_col) and (np.issubdtype(ddf[col].dtype, np.number))]\n",
    "    \n",
    "    # Filtrar booleanas\n",
    "    non_boolean = []\n",
    "    for col in numeric_cols:\n",
    "        sample = ddf[col].head(500)\n",
    "        if not is_boolean_column(sample):\n",
    "            non_boolean.append(col)\n",
    "    \n",
    "    print(f\"Columnas válidas: {len(non_boolean)}/{len(numeric_cols)}\")\n",
    "    \n",
    "    # Procesamiento en grupos\n",
    "    col_groups = [non_boolean[i:i+COLS_PER_GROUP] \n",
    "                 for i in range(0, len(non_boolean), COLS_PER_GROUP)]\n",
    "    \n",
    "    # Procesar por particiones\n",
    "    for partition_idx in range(ddf.npartitions):\n",
    "        print(f\"\\nPartición {partition_idx+1}/{ddf.npartitions}\")\n",
    "        partition_df = ddf.get_partition(partition_idx).compute()\n",
    "        target_values = partition_df[target_col].astype(DTYPE).values\n",
    "        \n",
    "        all_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for group_idx, cols in enumerate(col_groups):\n",
    "            print(f\"Grupo {group_idx+1}/{len(col_groups)}\", end='\\r')\n",
    "            \n",
    "            # Generar características (6 transformaciones por columna)\n",
    "            features = process_feature_group(partition_df, cols, target_col)\n",
    "            names = [f\"{col}_{suf}\" for col in cols \n",
    "                    for suf in ['orig', 'log', 'sqrt', 'squared', 'cubic', 'interact']]\n",
    "            \n",
    "            all_features.append(features)\n",
    "            feature_names.extend(names)\n",
    "        \n",
    "        # Crear DataFrame temporal con target\n",
    "        temp_df = pd.DataFrame(np.hstack(all_features), columns=feature_names, dtype=DTYPE)\n",
    "        temp_df[target_col] = target_values\n",
    "        \n",
    "        # Guardar en disco\n",
    "        temp_path = os.path.join(output_folder, f\"temp_part_{partition_idx}.parquet\")\n",
    "        temp_df.to_parquet(temp_path)\n",
    "        \n",
    "        del temp_df, partition_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # =============================================\n",
    "    # Selección de características\n",
    "    # =============================================\n",
    "    selectors = {\n",
    "        \"Linear\": f_classif,\n",
    "        \"Nonlinear\": mutual_info_classif,\n",
    "        \"Model-Based\": RandomForestClassifier(n_estimators=30, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    for sel_name, selector in selectors.items():\n",
    "        print(f\"\\n[SELECTOR] {sel_name}\")\n",
    "        \n",
    "        full_ddf, sample, X, y = None, None, None, None\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos\n",
    "            full_ddf = dd.read_parquet(os.path.join(output_folder, \"temp_part_*.parquet\"))\n",
    "            \n",
    "            # Muestreo para reducir memoria\n",
    "            sample = full_ddf.sample(frac=SAMPLE_FRACTION).compute()\n",
    "            X = sample.drop(target_col, axis=1)\n",
    "            y = sample[target_col]\n",
    "            \n",
    "            # Limpieza final\n",
    "            X = clean_nan(X)\n",
    "            \n",
    "            # Verificar NaN\n",
    "            if X.isna().sum().sum() > 0:\n",
    "                raise ValueError(f\"NaN residuales: {X.isna().sum().sum()}\")\n",
    "            \n",
    "            # Selección de características\n",
    "            if sel_name == \"Model-Based\":\n",
    "                model = selector.fit(X.sample(SAMPLE_SIZE_MODEL), y.sample(SAMPLE_SIZE_MODEL))\n",
    "                scores = pd.Series(model.feature_importances_, index=X.columns)\n",
    "            else:\n",
    "                with np.errstate(all='ignore'):\n",
    "                    scores = pd.Series(selector(X, y)[0], index=X.columns)\n",
    "            \n",
    "            # Seleccionar y guardar resultados\n",
    "            scores = scores.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            top_features = scores.nlargest(MAX_FEATURES).index.tolist()\n",
    "            \n",
    "            # Guardar reporte CSV\n",
    "            report_df = pd.DataFrame({'Feature': top_features, 'Score': scores[top_features].values})\n",
    "            report_path = os.path.join(output_folder, f\"{norm_name}_{sel_name}_report.csv\")\n",
    "            report_df.to_csv(report_path, index=False)\n",
    "            print(f\"✅ Reporte CSV guardado: {report_path}\")\n",
    "            \n",
    "            # Guardar dataset final\n",
    "            final_df = full_ddf[top_features + [target_col]].compute()\n",
    "            final_path = os.path.join(output_folder, f\"{norm_name}_{sel_name}_final.parquet\")\n",
    "            final_df.to_parquet(final_path, engine='pyarrow', compression='snappy')\n",
    "            print(f\"✅ Dataset Parquet guardado: {final_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en {sel_name}: {str(e)}\")\n",
    "        finally:\n",
    "            for var in ['full_ddf', 'sample', 'X', 'y']:\n",
    "                if var in locals():\n",
    "                    del locals()[var]\n",
    "            gc.collect()\n",
    "    \n",
    "    # Limpiar temporales\n",
    "    for f in os.listdir(output_folder):\n",
    "        if f.startswith(\"temp_\"):\n",
    "            os.remove(os.path.join(output_folder, f))\n",
    "\n",
    "def main():\n",
    "    experiment_name = \"experimento_final\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Configuración de paths\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    config_path = os.path.join(base_path, \"config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    normalized_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"normalized\"])\n",
    "    outputs_path = os.path.join(base_path, config[\"paths\"][\"outputs\"])\n",
    "    experiment_folder = os.path.join(outputs_path, \"experiments\", f\"{experiment_name}_{timestamp}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    # Procesar datasets\n",
    "    datasets = {\n",
    "        \"MaxAbs\": os.path.join(normalized_path, \"02_df_Maxabs.parquet\"),\n",
    "        \"MinMax\": os.path.join(normalized_path, \"02_df_MinMax.parquet\"),\n",
    "        \"NoNorm\": os.path.join(normalized_path, \"02_df_None.parquet\"),\n",
    "        \"Robust\": os.path.join(normalized_path, \"02_df_Robust.parquet\"),\n",
    "        \"Standard\": os.path.join(normalized_path, \"02_df_Standard.parquet\")\n",
    "    }\n",
    "    \n",
    "    for norm_name, path in datasets.items():\n",
    "        process_dataset(path, norm_name, experiment_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando MaxAbs\n",
      "Memoria inicial: 5.13 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 22/22\n",
      "[SELECTOR] Linear\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MaxAbs\\MaxAbs_Linear_final.parquet ((560486, 425))\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MaxAbs\\MaxAbs_Nonlinear_final.parquet ((560486, 425))\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MaxAbs\\MaxAbs_Model-Based_final.parquet ((560486, 425))\n",
      "\n",
      "Procesando MinMax\n",
      "Memoria inicial: 3.11 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "Grupo 22/22\n",
      "[SELECTOR] Linear\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MinMax\\MinMax_Linear_final.parquet ((560486, 425))\n",
      "\n",
      "[SELECTOR] Nonlinear\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MinMax\\MinMax_Nonlinear_final.parquet ((560486, 425))\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250129_174201\\MinMax\\MinMax_Model-Based_final.parquet ((560486, 425))\n",
      "\n",
      "Procesando NoNorm\n",
      "Memoria inicial: 3.22 GB\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/Administrador/Documents/PythonScripts/Tesis/tesisaustral/intermediate/normalized/02_df_No_Norm.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:5371\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, columns, filters, categories, index, storage_options, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, engine, arrow_to_pandas, **kwargs)\u001b[0m\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[0;32m   5353\u001b[0m         ReadParquetPyarrowFS(\n\u001b[0;32m   5354\u001b[0m             path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5367\u001b[0m         )\n\u001b[0;32m   5368\u001b[0m     )\n\u001b[0;32m   5370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m-> 5371\u001b[0m     \u001b[43mReadParquetFSSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_divisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5381\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5383\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_set_parquet_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5390\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_core.py:59\u001b[0m, in \u001b[0;36mExpr.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m inst\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;241m=\u001b[39m [_unpack_collections(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[1;32m---> 59\u001b[0m _name \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name \u001b[38;5;129;01min\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:995\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 995\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:780\u001b[0m, in \u001b[0;36mReadParquet._name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    777\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_funcname\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;241m+\u001b[39m _tokenize_deterministic(\n\u001b[1;32m--> 780\u001b[0m             funcname(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecksum\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    781\u001b[0m         )\n\u001b[0;32m    782\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:786\u001b[0m, in \u001b[0;36mReadParquet.checksum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchecksum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_info\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecksum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:1365\u001b[0m, in \u001b[0;36mReadParquetFSSpec._dataset_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1347\u001b[0m     paths,\n\u001b[0;32m   1348\u001b[0m     fs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1363\u001b[0m     },\n\u001b[0;32m   1364\u001b[0m )\n\u001b[1;32m-> 1365\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1366\u001b[0m checksum \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:992\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[1;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 992\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_wrapped_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_processed_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\pyarrow\\dataset.py:797\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[1;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\pyarrow\\dataset.py:474\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 474\u001b[0m         fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_multiple_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\pyarrow\\dataset.py:382\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[1;34m(paths, filesystem)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: c:/Users/Administrador/Documents/PythonScripts/Tesis/tesisaustral/intermediate/normalized/02_df_No_Norm.parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 222\u001b[0m\n\u001b[0;32m    219\u001b[0m         process_dataset(path, norm_name, experiment_folder)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 219\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    210\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaxAbs\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(normalized_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m02_df_Maxabs.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMax\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(normalized_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m02_df_MinMax.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(normalized_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m02_df_Std.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m }\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m norm_name, path \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 219\u001b[0m     \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 78\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(dataset_path, norm_name, experiment_folder)\u001b[0m\n\u001b[0;32m     75\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Carga de datos con Dask\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnivel_triage\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Identificar columnas numéricas (excluyendo target)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\dask\\backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: c:/Users/Administrador/Documents/PythonScripts/Tesis/tesisaustral/intermediate/normalized/02_df_No_Norm.parquet"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================\n",
    "# Configuración crítica de rendimiento\n",
    "# =============================================\n",
    "MAX_FEATURES = 500\n",
    "COLS_PER_GROUP = 5\n",
    "DTYPE = np.float32\n",
    "SAMPLE_SIZE_MODEL = 50000\n",
    "SAMPLE_FRACTION = 0.3\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Obtiene el uso de memoria actual en GB.\"\"\"\n",
    "    import psutil\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "\n",
    "def is_boolean_column(series):\n",
    "    \"\"\"Verifica si una columna es booleana (0/1).\"\"\"\n",
    "    unique_values = series.dropna().unique()\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "def safe_transform(data, transform_fn):\n",
    "    \"\"\"Aplica transformaciones numéricas seguras.\"\"\"\n",
    "    with np.errstate(all='ignore'):\n",
    "        result = transform_fn(np.abs(data))\n",
    "    return np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0).astype(DTYPE)\n",
    "\n",
    "def process_feature_group(df, cols_group, target_col):\n",
    "    \"\"\"Procesa características excluyendo el target.\"\"\"\n",
    "    features = []\n",
    "    for col in cols_group:\n",
    "        # Excluir el target de las transformaciones\n",
    "        if col == target_col:\n",
    "            continue\n",
    "            \n",
    "        col_data = df[col].astype(DTYPE).values\n",
    "        \n",
    "        if not is_boolean_column(df[col]):\n",
    "            target_data = df[target_col].astype(DTYPE).values  # Usar target para interacción\n",
    "            \n",
    "            features.extend([\n",
    "                col_data,\n",
    "                safe_transform(col_data, np.log1p),\n",
    "                safe_transform(col_data, np.sqrt),\n",
    "                col_data * target_data  # Interacción\n",
    "            ])\n",
    "        else:\n",
    "            features.append(col_data)\n",
    "    \n",
    "    return np.column_stack(features)\n",
    "\n",
    "def clean_nan(df):\n",
    "    \"\"\"Limpieza agresiva de NaN.\"\"\"\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median(skipna=True)\n",
    "            df[col] = df[col].fillna(median_val if not np.isnan(median_val) else 0.0)\n",
    "    return df\n",
    "\n",
    "def process_dataset(dataset_path, norm_name, experiment_folder):\n",
    "    print(f\"\\nProcesando {norm_name}\")\n",
    "    print(f\"Memoria inicial: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Configuración de rutas\n",
    "    output_folder = os.path.join(experiment_folder, norm_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Carga de datos con Dask\n",
    "    ddf = dd.read_parquet(dataset_path)\n",
    "    target_col = 'nivel_triage'\n",
    "    \n",
    "    # Identificar columnas numéricas (excluyendo target)\n",
    "    numeric_cols = [col for col in ddf.columns \n",
    "                   if (col != target_col) and (np.issubdtype(ddf[col].dtype, np.number))]\n",
    "    \n",
    "    # Filtrar booleanas\n",
    "    non_boolean = []\n",
    "    for col in numeric_cols:\n",
    "        sample = ddf[col].head(500)\n",
    "        if not is_boolean_column(sample):\n",
    "            non_boolean.append(col)\n",
    "    \n",
    "    print(f\"Columnas válidas: {len(non_boolean)}/{len(numeric_cols)}\")\n",
    "    \n",
    "    # Procesamiento en grupos\n",
    "    col_groups = [non_boolean[i:i+COLS_PER_GROUP] \n",
    "                 for i in range(0, len(non_boolean), COLS_PER_GROUP)]\n",
    "    \n",
    "    # Procesar por particiones\n",
    "    for partition_idx in range(ddf.npartitions):\n",
    "        print(f\"\\nPartición {partition_idx+1}/{ddf.npartitions}\")\n",
    "        partition_df = ddf.get_partition(partition_idx).compute()\n",
    "        target_values = partition_df[target_col].astype(DTYPE).values  # Guardar target\n",
    "        \n",
    "        all_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for group_idx, cols in enumerate(col_groups):\n",
    "            print(f\"Grupo {group_idx+1}/{len(col_groups)}\", end='\\r')\n",
    "            \n",
    "            # Generar características (excluyendo target)\n",
    "            features = process_feature_group(partition_df, cols, target_col)\n",
    "            names = [f\"{col}_{suf}\" for col in cols \n",
    "                    for suf in ['orig', 'log', 'sqrt', 'interact']]\n",
    "            \n",
    "            all_features.append(features)\n",
    "            feature_names.extend(names)\n",
    "        \n",
    "        # Crear DataFrame temporal CON target\n",
    "        temp_df = pd.DataFrame(np.hstack(all_features), columns=feature_names, dtype=DTYPE)\n",
    "        temp_df[target_col] = target_values  # Añadir target sin transformar\n",
    "        \n",
    "        # Guardar en disco\n",
    "        temp_path = os.path.join(output_folder, f\"temp_part_{partition_idx}.parquet\")\n",
    "        temp_df.to_parquet(temp_path)\n",
    "        \n",
    "        del temp_df, partition_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # =============================================\n",
    "    # Selección de características (excluyendo target)\n",
    "    # =============================================\n",
    "    selectors = {\n",
    "        \"Linear\": f_classif,\n",
    "        \"Nonlinear\": mutual_info_classif,\n",
    "        \"Model-Based\": RandomForestClassifier(n_estimators=30, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    for sel_name, selector in selectors.items():\n",
    "        print(f\"\\n[SELECTOR] {sel_name}\")\n",
    "        \n",
    "        # Variables inicializadas para evitar errores\n",
    "        full_ddf, sample, X, y = None, None, None, None\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos\n",
    "            full_ddf = dd.read_parquet(os.path.join(output_folder, \"temp_part_*.parquet\"))\n",
    "            \n",
    "            # Muestreo para reducir memoria\n",
    "            sample = full_ddf.sample(frac=SAMPLE_FRACTION).compute()\n",
    "            X = sample.drop(target_col, axis=1)  # Excluir target de features\n",
    "            y = sample[target_col]               # Target separado\n",
    "            \n",
    "            # Limpieza final\n",
    "            X = clean_nan(X)\n",
    "            \n",
    "            # Verificar NaN\n",
    "            if X.isna().sum().sum() > 0:\n",
    "                raise ValueError(f\"NaN residuales: {X.isna().sum().sum()}\")\n",
    "            \n",
    "            # Selección de características\n",
    "            if sel_name == \"Model-Based\":\n",
    "                model = selector.fit(X.sample(SAMPLE_SIZE_MODEL), y.sample(SAMPLE_SIZE_MODEL))\n",
    "                scores = pd.Series(model.feature_importances_, index=X.columns)\n",
    "            else:\n",
    "                with np.errstate(all='ignore'):\n",
    "                    scores = pd.Series(selector(X, y)[0], index=X.columns)\n",
    "            \n",
    "            # Seleccionar top features\n",
    "            scores = scores.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "            top_features = scores.nlargest(MAX_FEATURES).index.tolist()\n",
    "            \n",
    "            # Guardar dataset final CON target\n",
    "            final_df = full_ddf[top_features + [target_col]].compute()\n",
    "            \n",
    "            output_path = os.path.join(output_folder, f\"{norm_name}_{sel_name}_final.parquet\")\n",
    "            final_df.to_parquet(output_path, engine='pyarrow', compression='snappy')\n",
    "            print(f\"✅ Guardado: {output_path} ({final_df.shape})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en {sel_name}: {str(e)}\")\n",
    "        finally:\n",
    "            # Limpieza segura de variables\n",
    "            for var in ['full_ddf', 'sample', 'X', 'y']:\n",
    "                if var in locals():\n",
    "                    del locals()[var]\n",
    "            gc.collect()\n",
    "    \n",
    "    # Limpiar temporales\n",
    "    for f in os.listdir(output_folder):\n",
    "        if f.startswith(\"temp_\"):\n",
    "            os.remove(os.path.join(output_folder, f))\n",
    "\n",
    "def main():\n",
    "    experiment_name = \"experimento_final\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Configuración de paths\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    config_path = os.path.join(base_path, \"config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    normalized_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"normalized\"])\n",
    "    outputs_path = os.path.join(base_path, config[\"paths\"][\"outputs\"])\n",
    "    experiment_folder = os.path.join(outputs_path, \"experiments\", f\"{experiment_name}_{timestamp}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    # Procesar datasets\n",
    "    datasets = {\n",
    "        \"MaxAbs\": os.path.join(normalized_path, \"02_df_Maxabs.parquet\"),\n",
    "        \"MinMax\": os.path.join(normalized_path, \"02_df_MinMax.parquet\"),\n",
    "        \"NoNorm\": os.path.join(normalized_path, \"02_df_None.parquet\"),\n",
    "        \"Robust\": os.path.join(normalized_path, \"02_df_Robust.parquet\"),\n",
    "        \"Standard\": os.path.join(normalized_path, \"02_df_Std.parquet\")\n",
    "    }\n",
    "    \n",
    "    for norm_name, path in datasets.items():\n",
    "        process_dataset(path, norm_name, experiment_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

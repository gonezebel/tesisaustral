{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando MaxAbs\n",
      "Memoria inicial: 0.21 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "\n",
      "Consolidating full datasets...\n",
      "✅ Consolidated full dataset saved to: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MaxAbs\\MaxAbs_full.parquet\n",
      "\n",
      "[SELECTOR] Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [632 633] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MaxAbs\\MaxAbs_Linear_selected.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MaxAbs\\MaxAbs_Nonlinear_selected.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MaxAbs\\MaxAbs_Model-Based_selected.parquet\n",
      "\n",
      "Procesando MinMax\n",
      "Memoria inicial: 4.32 GB\n",
      "Columnas válidas: 106/567\n",
      "\n",
      "Partición 1/1\n",
      "\n",
      "Consolidating full datasets...\n",
      "✅ Consolidated full dataset saved to: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MinMax\\MinMax_full.parquet\n",
      "\n",
      "[SELECTOR] Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [632 633] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MinMax\\MinMax_Linear_selected.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MinMax\\MinMax_Nonlinear_selected.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\MinMax\\MinMax_Model-Based_selected.parquet\n",
      "\n",
      "Procesando NoNorm\n",
      "Memoria inicial: 4.82 GB\n",
      "Columnas válidas: 90/567\n",
      "\n",
      "Partición 1/1\n",
      "\n",
      "Consolidating full datasets...\n",
      "✅ Consolidated full dataset saved to: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\NoNorm\\NoNorm_full.parquet\n",
      "\n",
      "[SELECTOR] Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [536 537] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\NoNorm\\NoNorm_Linear_selected.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\NoNorm\\NoNorm_Nonlinear_selected.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\NoNorm\\NoNorm_Model-Based_selected.parquet\n",
      "\n",
      "Procesando Robust\n",
      "Memoria inicial: 4.97 GB\n",
      "Columnas válidas: 90/567\n",
      "\n",
      "Partición 1/1\n",
      "\n",
      "Consolidating full datasets...\n",
      "✅ Consolidated full dataset saved to: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Robust\\Robust_full.parquet\n",
      "\n",
      "[SELECTOR] Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [536 537] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Robust\\Robust_Linear_selected.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Robust\\Robust_Nonlinear_selected.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Robust\\Robust_Model-Based_selected.parquet\n",
      "\n",
      "Procesando Standard\n",
      "Memoria inicial: 4.74 GB\n",
      "Columnas válidas: 112/567\n",
      "\n",
      "Partición 1/1\n",
      "\n",
      "Consolidating full datasets...\n",
      "✅ Consolidated full dataset saved to: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Standard\\Standard_full.parquet\n",
      "\n",
      "[SELECTOR] Linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [668 669] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Standard\\Standard_Linear_selected.parquet\n",
      "\n",
      "[SELECTOR] Nonlinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Standard\\Standard_Nonlinear_selected.parquet\n",
      "\n",
      "[SELECTOR] Model-Based\n",
      "✅ Dataset seleccionado guardado: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250201_151905\\Standard\\Standard_Model-Based_selected.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuración global\n",
    "DTYPE = np.float32\n",
    "COLS_PER_GROUP = 50\n",
    "SAMPLE_FRACTION = 0.2\n",
    "SAMPLE_SIZE_MODEL = 10000\n",
    "CUMULATIVE_THRESHOLD = 0.95\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Obtiene el uso de memoria actual en GB.\"\"\"\n",
    "    import psutil\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "\n",
    "def is_boolean_column(series):\n",
    "    \"\"\"Verifica si una columna es booleana (0/1).\"\"\"\n",
    "    unique_values = series.dropna().unique()\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "def safe_transform(data, transform_fn):\n",
    "    \"\"\"Aplica transformaciones numéricas seguras.\"\"\"\n",
    "    with np.errstate(all='ignore'):\n",
    "        result = transform_fn(np.abs(data))\n",
    "    return np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0).astype(DTYPE)\n",
    "\n",
    "def process_feature_group(df, cols_group, target_col):\n",
    "    \"\"\"Procesa características incluyendo transformaciones.\"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    for col in cols_group:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "            \n",
    "        col_data = df[col].astype(DTYPE).values\n",
    "        \n",
    "        if not is_boolean_column(pd.Series(col_data)):\n",
    "            target_data = df[target_col].astype(DTYPE).values\n",
    "            \n",
    "            transformations = [\n",
    "                (col_data, f\"{col}_orig\"),\n",
    "                (safe_transform(col_data, np.log1p), f\"{col}_log\"),\n",
    "                (safe_transform(col_data, np.sqrt), f\"{col}_sqrt\"),\n",
    "                (col_data ** 2, f\"{col}_squared\"),\n",
    "                (col_data ** 3, f\"{col}_cubic\"),\n",
    "                (col_data * target_data, f\"{col}_interact\")\n",
    "            ]\n",
    "            \n",
    "            for data, name in transformations:\n",
    "                features.append(data)\n",
    "                feature_names.append(name)\n",
    "        else:\n",
    "            features.append(col_data)\n",
    "            feature_names.append(f\"{col}_orig\")\n",
    "    \n",
    "    return np.column_stack(features), feature_names\n",
    "\n",
    "def clean_nan(df):\n",
    "    \"\"\"Limpieza agresiva de NaN.\"\"\"\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median(skipna=True)\n",
    "            df[col] = df[col].fillna(median_val if not np.isnan(median_val) else 0.0)\n",
    "    return df.astype(DTYPE)\n",
    "\n",
    "def find_optimal_cutoff(cumulative_importance, min_features=10):\n",
    "    \"\"\"Encuentra el punto de corte óptimo combinando métodos.\"\"\"\n",
    "    # Método 1: Umbral acumulado\n",
    "    threshold_idx = np.where(cumulative_importance >= CUMULATIVE_THRESHOLD)[0]\n",
    "    threshold_idx = threshold_idx[0] if len(threshold_idx) > 0 else len(cumulative_importance)\n",
    "    \n",
    "    # Método 2: Punto de inflexión\n",
    "    npoints = len(cumulative_importance)\n",
    "    if npoints < 2:\n",
    "        return min_features\n",
    "    \n",
    "    all_coords = np.vstack((range(npoints), cumulative_importance)).T\n",
    "    vec_line = all_coords[-1] - all_coords[0]\n",
    "    vec_norm = np.array([-vec_line[1], vec_line[0]])\n",
    "    vec_norm = vec_norm / np.sqrt(np.sum(vec_norm**2))\n",
    "    vec_from_first = all_coords - all_coords[0]\n",
    "    distances = np.abs(np.dot(vec_from_first, vec_norm))\n",
    "    \n",
    "    try:\n",
    "        elbow_idx = np.nanargmax(distances)\n",
    "    except ValueError:\n",
    "        elbow_idx = min_features\n",
    "    \n",
    "    return max(min_features, min(threshold_idx, elbow_idx))\n",
    "\n",
    "def analyze_feature_importance_enhanced(scores, min_features=10):\n",
    "    \"\"\"Analiza la importancia de características con método combinado.\"\"\"\n",
    "    sorted_scores = scores.sort_values(ascending=False)\n",
    "    \n",
    "    if sorted_scores.empty or sorted_scores.sum() == 0:\n",
    "        return [], pd.DataFrame(), 0\n",
    "    \n",
    "    cumulative = sorted_scores.cumsum() / sorted_scores.sum()\n",
    "    cutoff_idx = find_optimal_cutoff(cumulative.values, min_features)\n",
    "    \n",
    "    analysis_df = pd.DataFrame({\n",
    "        'feature': sorted_scores.index,\n",
    "        'importance_score': sorted_scores.values,\n",
    "        'cumulative_importance': cumulative.values,\n",
    "        'rank': range(1, len(sorted_scores)+1)\n",
    "    })\n",
    "    \n",
    "    selected_features = analysis_df.iloc[:cutoff_idx]['feature'].tolist()\n",
    "    return selected_features, analysis_df, cutoff_idx\n",
    "\n",
    "def plot_importance_analysis(analysis_df, cutoff_idx, plot_path):\n",
    "    \"\"\"Genera gráficos de importancia mejorados.\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Gráfico de barras para top 50\n",
    "    plt.subplot(2, 1, 1)\n",
    "    top_features = analysis_df.head(50)\n",
    "    plt.bar(range(len(top_features)), top_features['importance_score'], color='darkblue')\n",
    "    if cutoff_idx < 50:\n",
    "        plt.axvline(x=cutoff_idx, color='r', linestyle='--', label=f'Punto de corte: {cutoff_idx}')\n",
    "    plt.title('Top 50 Características por Importancia')\n",
    "    plt.xlabel('Ranking')\n",
    "    plt.ylabel('Score de Importancia')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gráfico de importancia acumulada\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(analysis_df['cumulative_importance'], 'g-')\n",
    "    plt.axvline(cutoff_idx, color='r', linestyle='--', label=f'Punto de corte: {cutoff_idx}')\n",
    "    plt.axhline(CUMULATIVE_THRESHOLD, color='orange', linestyle=':', label=f'Umbral {CUMULATIVE_THRESHOLD}')\n",
    "    plt.title('Importancia Acumulada')\n",
    "    plt.xlabel('Número de Características')\n",
    "    plt.ylabel('Importancia Acumulada')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def process_dataset(dataset_path, norm_name, experiment_folder):\n",
    "    print(f\"\\nProcesando {norm_name}\")\n",
    "    print(f\"Memoria inicial: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "    output_folder = os.path.join(experiment_folder, norm_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        ddf = dd.read_parquet(dataset_path).reset_index(drop=True)\n",
    "        target_col = 'nivel_triage'\n",
    "\n",
    "        numeric_cols = [col for col in ddf.columns\n",
    "                        if (col != target_col) and (np.issubdtype(ddf[col].dtype, np.number))]\n",
    "\n",
    "        non_boolean = []\n",
    "        for col in numeric_cols:\n",
    "            sample = ddf[col].head(500)\n",
    "            if not is_boolean_column(sample):\n",
    "                non_boolean.append(col)\n",
    "\n",
    "        print(f\"Columnas válidas: {len(non_boolean)}/{len(numeric_cols)}\")\n",
    "\n",
    "        col_groups = [non_boolean[i:i+COLS_PER_GROUP]\n",
    "                      for i in range(0, len(non_boolean), COLS_PER_GROUP)]\n",
    "\n",
    "        full_partition_paths = []  # List to store paths of partition files\n",
    "\n",
    "        for partition_idx in range(ddf.npartitions):\n",
    "            print(f\"\\nPartición {partition_idx+1}/{ddf.npartitions}\")\n",
    "            partition_df = ddf.get_partition(partition_idx).compute().reset_index(drop=True)\n",
    "\n",
    "            all_features = []\n",
    "            all_names = []\n",
    "\n",
    "            for cols in col_groups:\n",
    "                features, names = process_feature_group(partition_df, cols, target_col)\n",
    "                all_features.append(features)\n",
    "                all_names.extend(names)\n",
    "\n",
    "            full_df = pd.DataFrame(\n",
    "                np.hstack(all_features),\n",
    "                columns=all_names,\n",
    "                dtype=DTYPE\n",
    "            )\n",
    "            full_df[target_col] = partition_df[target_col].astype(DTYPE).values\n",
    "\n",
    "            full_path = os.path.join(output_folder, f\"{norm_name}_full_part_{partition_idx}.parquet\")\n",
    "            full_df.to_parquet(full_path, index=False)\n",
    "            full_partition_paths.append(full_path)  # Add path to list\n",
    "\n",
    "            del full_df, partition_df\n",
    "            gc.collect()\n",
    "\n",
    "        # ***NEW CODE TO CONSOLIDATE FULL DATASETS***\n",
    "        print(\"\\nConsolidating full datasets...\")\n",
    "        full_ddf = dd.read_parquet(full_partition_paths)\n",
    "        final_full_path = os.path.join(output_folder, f\"{norm_name}_full.parquet\")\n",
    "        full_ddf.to_parquet(final_full_path, write_index=False) # Save the combined full dataset\n",
    "        print(f\"✅ Consolidated full dataset saved to: {final_full_path}\")\n",
    "\n",
    "\n",
    "        selectors = {\n",
    "            \"Linear\": f_classif,\n",
    "            \"Nonlinear\": mutual_info_classif,\n",
    "            \"Model-Based\": RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)\n",
    "        }\n",
    "\n",
    "        for sel_name, selector in selectors.items():\n",
    "            print(f\"\\n[SELECTOR] {sel_name}\")\n",
    "\n",
    "            try:\n",
    "                # Load the *consolidated* full dataset\n",
    "                full_ddf = dd.read_parquet(final_full_path)  # Changed this line\n",
    "\n",
    "                sample = full_ddf.sample(frac=SAMPLE_FRACTION).compute()\n",
    "                X = clean_nan(sample.drop(target_col, axis=1))\n",
    "                y = sample[target_col].astype(int)\n",
    "\n",
    "                if X.empty:\n",
    "                    raise ValueError(\"No hay características válidas después de la limpieza\")\n",
    "\n",
    "                if sel_name == \"Model-Based\":\n",
    "                    X_sample, _, y_sample, _ = train_test_split(\n",
    "                        X, y,\n",
    "                        stratify=y,\n",
    "                        train_size=SAMPLE_SIZE_MODEL,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model = selector.fit(X_sample, y_sample)\n",
    "                    scores = pd.Series(model.feature_importances_, index=X.columns)\n",
    "                else:\n",
    "                    with np.errstate(all='ignore'):\n",
    "                        scores_values = selector(X, y)\n",
    "                        scores = pd.Series(scores_values[0] if isinstance(scores_values, tuple) else scores_values,\n",
    "                                          index=X.columns)\n",
    "\n",
    "                scores = scores.replace([np.inf, -np.inf], np.nan)\n",
    "                scores = scores.fillna(scores.mean() if not scores.isna().all() else 0)\n",
    "\n",
    "                selected_features, analysis_df, cutoff_idx = analyze_feature_importance_enhanced(scores)\n",
    "\n",
    "                analysis_df.to_csv(os.path.join(output_folder, f\"{norm_name}_{sel_name}_report.csv\"), index=False)\n",
    "                plot_importance_analysis(analysis_df, cutoff_idx, os.path.join(output_folder, f\"{norm_name}_{sel_name}_plot.png\"))\n",
    "\n",
    "                selected_df = full_ddf[selected_features + [target_col]].compute()\n",
    "                selected_path = os.path.join(output_folder, f\"{norm_name}_{sel_name}_selected.parquet\")\n",
    "                selected_df.to_parquet(selected_path, index=False)\n",
    "                print(f\"✅ Dataset seleccionado guardado: {selected_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error en {sel_name}: {str(e)}\")\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "        # Limpiar archivos temporales *después* de consolidar\n",
    "        for f in os.listdir(output_folder):\n",
    "            if \"_full_part_\" in f:\n",
    "                os.remove(os.path.join(output_folder, f))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error crítico: {str(e)}\")\n",
    "def main():\n",
    "    experiment_name = \"experimento_final\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Configuración de paths\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    config_path = os.path.join(base_path, \"config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    normalized_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"normalized\"])\n",
    "    outputs_path = os.path.join(base_path, config[\"paths\"][\"outputs\"])\n",
    "    experiment_folder = os.path.join(outputs_path, \"experiments\", f\"{experiment_name}_{timestamp}\")\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    datasets = {\n",
    "        \"MaxAbs\": \"02_df_Maxabs.parquet\",\n",
    "        \"MinMax\": \"02_df_MinMax.parquet\",\n",
    "        \"NoNorm\": \"02_df_None.parquet\",\n",
    "        \"Robust\": \"02_df_Robust.parquet\",\n",
    "        \"Standard\": \"02_df_Standard.parquet\"\n",
    "    }\n",
    "    \n",
    "    for norm_name, file in datasets.items():\n",
    "        dataset_path = os.path.join(normalized_path, file)\n",
    "        if os.path.exists(dataset_path):\n",
    "            process_dataset(dataset_path, norm_name, experiment_folder)\n",
    "        else:\n",
    "            print(f\"❌ Dataset no encontrado: {dataset_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando MaxAbs\n",
      "\n",
      "Procesando MinMax\n",
      "\n",
      "Procesando NoNorm\n",
      "\n",
      "Procesando Robust\n",
      "\n",
      "Procesando Standard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuración global\n",
    "DTYPE = np.float32\n",
    "SAMPLE_FRACTION = 0.2\n",
    "SAMPLE_SIZE_MODEL = 10000\n",
    "CUMULATIVE_THRESHOLD = 0.95\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Obtiene el uso de memoria actual en GB.\"\"\"\n",
    "    import psutil\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 ** 3\n",
    "\n",
    "def clean_nan(df):\n",
    "    \"\"\"Limpieza agresiva de NaN optimizada.\"\"\"\n",
    "    # Optimización: Usar máscaras booleanas en lugar de operaciones iterativas\n",
    "    null_counts = df.isnull().sum()\n",
    "    cols_to_keep = null_counts[null_counts < len(df)].index\n",
    "    df = df[cols_to_keep]\n",
    "    \n",
    "    # Optimización: Vectorizar el reemplazo de NaN\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    df = df.fillna(0.0)\n",
    "    \n",
    "    return df.astype(DTYPE)\n",
    "\n",
    "def find_optimal_cutoff(cumulative_importance, min_features=10):\n",
    "    \"\"\"Encuentra el punto de corte óptimo (optimizado).\"\"\"\n",
    "    threshold_idx = np.where(cumulative_importance >= CUMULATIVE_THRESHOLD)[0]\n",
    "    threshold_idx = threshold_idx[0] if len(threshold_idx) > 0 else len(cumulative_importance)\n",
    "    \n",
    "    if len(cumulative_importance) < 2:\n",
    "        return min_features\n",
    "    \n",
    "    # Optimización: Cálculo vectorizado del punto de inflexión\n",
    "    x = np.arange(len(cumulative_importance))\n",
    "    y = cumulative_importance\n",
    "    slopes = np.diff(y) / np.diff(x)\n",
    "    elbow_idx = np.argmax(np.abs(np.diff(slopes))) + 1\n",
    "    \n",
    "    return max(min_features, min(threshold_idx, elbow_idx))\n",
    "\n",
    "def analyze_feature_importance_enhanced(scores, feature_names=None, min_features=10):\n",
    "    \"\"\"Análisis de importancia optimizado.\"\"\"\n",
    "    # Si scores no viene con nombres de características, crear índices\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(scores))]\n",
    "    \n",
    "    # Crear Series con los nombres de características correctos\n",
    "    scores = pd.Series(scores, index=feature_names)\n",
    "    \n",
    "    sorted_scores = scores.sort_values(ascending=False)\n",
    "    \n",
    "    if sorted_scores.empty or sorted_scores.sum() == 0:\n",
    "        return [], pd.DataFrame(), 0\n",
    "    \n",
    "    cumulative = np.cumsum(sorted_scores.values) / sorted_scores.sum()\n",
    "    cutoff_idx = find_optimal_cutoff(cumulative, min_features)\n",
    "    \n",
    "    analysis_df = pd.DataFrame({\n",
    "        'feature': sorted_scores.index,\n",
    "        'importance_score': sorted_scores.values,\n",
    "        'cumulative_importance': cumulative,\n",
    "        'rank': np.arange(1, len(sorted_scores) + 1)\n",
    "    })\n",
    "    \n",
    "    selected_features = analysis_df['feature'].iloc[:cutoff_idx].tolist()\n",
    "    return selected_features, analysis_df, cutoff_idx\n",
    "\n",
    "def plot_importance_analysis(analysis_df, cutoff_idx, plot_path):\n",
    "    \"\"\"Generación de gráficos optimizada.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    dynamic_num = min(max(int(cutoff_idx * 1.25), 10), len(analysis_df))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    top_features = analysis_df.head(dynamic_num)\n",
    "    bars = plt.bar(range(len(top_features)), top_features['importance_score'], color='darkblue')\n",
    "    \n",
    "    if cutoff_idx < dynamic_num:\n",
    "        plt.axvline(x=cutoff_idx, color='r', linestyle='--', linewidth=2, \n",
    "                   label=f'Punto de corte: {cutoff_idx}')\n",
    "    \n",
    "    plt.title(f'Top {dynamic_num} Características por Importancia')\n",
    "    plt.xlabel('Ranking')\n",
    "    plt.ylabel('Score de Importancia')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(analysis_df['cumulative_importance'], 'g-', linewidth=2)\n",
    "    plt.axvline(cutoff_idx, color='r', linestyle='--', linewidth=2, \n",
    "               label=f'Punto de corte: {cutoff_idx}')\n",
    "    plt.axhline(CUMULATIVE_THRESHOLD, color='orange', linestyle=':', \n",
    "               linewidth=2, label=f'Umbral {CUMULATIVE_THRESHOLD}')\n",
    "    \n",
    "    plt.title('Importancia Acumulada de Características')\n",
    "    plt.xlabel('Número de Características')\n",
    "    plt.ylabel('Importancia Acumulada')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_single_parquet(ddf, path):\n",
    "    \"\"\"Guarda un Dask DataFrame como un único archivo Parquet.\"\"\"\n",
    "    # Convertir a Pandas DataFrame si es necesario\n",
    "    if isinstance(ddf, dd.DataFrame):\n",
    "        ddf = ddf.compute()\n",
    "    ddf.to_parquet(path, index=False)\n",
    "\n",
    "def process_dataset(dataset_path, norm_name, experiment_folder):\n",
    "    print(f\"\\nProcesando {norm_name}\")\n",
    "    \n",
    "    output_folder = os.path.join(experiment_folder, norm_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        ddf = dd.read_parquet(dataset_path)\n",
    "        target_col = 'nivel_triage'\n",
    "        \n",
    "        sample = ddf.sample(frac=SAMPLE_FRACTION).compute()\n",
    "        X = clean_nan(sample.drop(columns=[target_col]))\n",
    "        y = sample[target_col].astype(int)\n",
    "\n",
    "        if X.empty:\n",
    "            raise ValueError(\"No hay características válidas después de la limpieza\")\n",
    "\n",
    "        feature_names = X.columns.tolist()\n",
    "\n",
    "        # Guardar un archivo Parquet con todas las variables (sin selección de características)\n",
    "        save_single_parquet(\n",
    "            ddf,\n",
    "            os.path.join(output_folder, f\"{norm_name}_all_features.parquet\")\n",
    "        )\n",
    "\n",
    "        # Modificación principal: Separar la lógica de los selectores\n",
    "        for sel_name in [\"Linear\", \"Nonlinear\", \"Model-Based\"]:\n",
    "            try:\n",
    "                # Obtener scores según el tipo de selector\n",
    "                if sel_name == \"Model-Based\":\n",
    "                    # Usar RandomForest\n",
    "                    X_sample, _, y_sample, _ = train_test_split(\n",
    "                        X, y,\n",
    "                        train_size=min(SAMPLE_SIZE_MODEL, len(X)),\n",
    "                        stratify=y,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model = RandomForestClassifier(\n",
    "                        n_estimators=50,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=42,\n",
    "                        max_depth=10\n",
    "                    )\n",
    "                    model.fit(X_sample, y_sample)\n",
    "                    scores = model.feature_importances_\n",
    "                \n",
    "                elif sel_name == \"Linear\":\n",
    "                    # Usar f_classif\n",
    "                    with np.errstate(all='ignore'):\n",
    "                        scores, _ = f_classif(X, y)\n",
    "                \n",
    "                else:  # Nonlinear\n",
    "                    # Usar mutual_info_classif\n",
    "                    with np.errstate(all='ignore'):\n",
    "                        scores = mutual_info_classif(X, y)\n",
    "\n",
    "                # Análisis de importancia\n",
    "                selected_features, analysis_df, cutoff_idx = analyze_feature_importance_enhanced(\n",
    "                    scores,\n",
    "                    feature_names=feature_names\n",
    "                )\n",
    "                \n",
    "                # Guardar resultados\n",
    "                analysis_df.to_csv(\n",
    "                    os.path.join(output_folder, f\"{norm_name}_{sel_name}_report.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                plot_importance_analysis(\n",
    "                    analysis_df,\n",
    "                    cutoff_idx,\n",
    "                    os.path.join(output_folder, f\"{norm_name}_{sel_name}_plot.png\")\n",
    "                )\n",
    "\n",
    "                selected_cols = selected_features + [target_col]\n",
    "                save_single_parquet(\n",
    "                    ddf[selected_cols],\n",
    "                    os.path.join(output_folder, f\"{norm_name}_{sel_name}_selected.parquet\")\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error en {sel_name}: {str(e)}\")\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error crítico: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    experiment_name = \"experimento_final\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    with open(os.path.join(base_path, \"config.json\")) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    normalized_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"normalized\"])\n",
    "    experiment_folder = os.path.join(\n",
    "        base_path, \n",
    "        config[\"paths\"][\"outputs\"], \n",
    "        \"experiments\", \n",
    "        f\"{experiment_name}_{timestamp}\"\n",
    "    )\n",
    "    os.makedirs(experiment_folder, exist_ok=True)\n",
    "    \n",
    "    datasets = {\n",
    "        \"MaxAbs\": \"df_feateng_MaxAbs.parquet\",\n",
    "        \"MinMax\": \"df_feateng_MinMax.parquet\",\n",
    "        \"NoNorm\": \"df_feateng_None.parquet\",\n",
    "        \"Robust\": \"df_feateng_Robust.parquet\",\n",
    "        \"Standard\": \"df_feateng_Standard.parquet\"\n",
    "    }\n",
    "    \n",
    "    for norm_name, file in datasets.items():\n",
    "        dataset_path = os.path.join(normalized_path, file)\n",
    "        if os.path.exists(dataset_path):\n",
    "            process_dataset(dataset_path, norm_name, experiment_folder)\n",
    "        else:\n",
    "            print(f\"❌ Dataset no encontrado: {dataset_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identificando columnas...\n",
      "Realizando imputación...\n",
      "Generando características...\n",
      "Generando interacciones...\n",
      "Seleccionando características...\n",
      "Generando reporte PDF...\n",
      "\n",
      "✅ Feature engineering completado.\n",
      "Dataset guardado en: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\intermediate/featured\\df_feateng.parquet\n",
      "Reporte PDF generado en: c:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\intermediate/featured\\feature_engineering_report.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from category_encoders import TargetEncoder\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "def is_boolean_column(series):\n",
    "    \"\"\"Detección rápida de columnas booleanas usando numpy\"\"\"\n",
    "    if pd.api.types.is_bool_dtype(series):\n",
    "        return True\n",
    "    # Convertir a numpy array para procesamiento más rápido\n",
    "    unique_values = pd.unique(series.dropna().to_numpy())\n",
    "    return set(unique_values).issubset({0, 1, 0.0, 1.0})\n",
    "\n",
    "def identify_columns_for_processing(df):\n",
    "    \"\"\"Identifica eficientemente las columnas para procesar\"\"\"\n",
    "    # Usar numpy para operaciones vectorizadas\n",
    "    column_types = {}\n",
    "    for col in df.columns:\n",
    "        if col == 'nivel_triage':\n",
    "            continue\n",
    "        \n",
    "        series = df[col]\n",
    "        if is_boolean_column(series):\n",
    "            column_types['boolean_cols'] = column_types.get('boolean_cols', []) + [col]\n",
    "        elif np.issubdtype(series.dtype, np.number):\n",
    "            column_types['numeric_cols'] = column_types.get('numeric_cols', []) + [col]\n",
    "    \n",
    "    return column_types\n",
    "\n",
    "def batch_safe_transformation(data, cols):\n",
    "    \"\"\"Aplica transformaciones en lotes usando operaciones vectorizadas\"\"\"\n",
    "    transformations = {}\n",
    "    \n",
    "    # Convertir a numpy array para operaciones más rápidas\n",
    "    data_array = data[cols].to_numpy()\n",
    "    \n",
    "    # Calcular máscaras una vez\n",
    "    non_negative_mask = (data_array >= 0).all(axis=0)\n",
    "    positive_mask = (data_array > 0).all(axis=0)\n",
    "    \n",
    "    # Aplicar transformaciones en lotes\n",
    "    for i, col in enumerate(cols):\n",
    "        if non_negative_mask[i]:\n",
    "            transformations[f\"{col}_log\"] = np.log1p(data_array[:, i])\n",
    "        if positive_mask[i]:\n",
    "            transformations[f\"{col}_sqrt\"] = np.sqrt(data_array[:, i])\n",
    "        \n",
    "        # Transformaciones seguras para cualquier distribución\n",
    "        transformations[f\"{col}_squared\"] = np.square(data_array[:, i])\n",
    "        \n",
    "        # Calcular z-score vectorizado\n",
    "        mean = np.mean(data_array[:, i])\n",
    "        std = np.std(data_array[:, i])\n",
    "        transformations[f\"{col}_zscore\"] = (data_array[:, i] - mean) / std\n",
    "    \n",
    "    return pd.DataFrame(transformations, index=data.index)\n",
    "\n",
    "def generate_safe_interactions(df, numeric_cols, target_col='nivel_triage', batch_size=1000):\n",
    "    \"\"\"Genera interacciones en lotes\"\"\"\n",
    "    encoder = TargetEncoder(cols=numeric_cols, smoothing=20)\n",
    "    \n",
    "    # Procesar en lotes para reducir uso de memoria\n",
    "    n_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    encoded_dfs = []\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        encoded_batch = encoder.fit_transform(batch_df[numeric_cols], batch_df[target_col])\n",
    "        interactions_batch = encoded_batch.mul(batch_df[target_col], axis=0)\n",
    "        encoded_dfs.append(interactions_batch)\n",
    "    \n",
    "    interactions = pd.concat(encoded_dfs)\n",
    "    interactions.columns = [f\"{col}_x_{target_col}\" for col in interactions.columns]\n",
    "    return interactions\n",
    "\n",
    "def get_column_summary(stage, df, column_types=None):\n",
    "    \"\"\"Genera un resumen mejorado de las columnas con información de tipos\"\"\"\n",
    "    summary = []\n",
    "    summary.append(f\"Resumen de columnas - {stage}\")\n",
    "    summary.append(f\"Total columnas: {len(df.columns)}\")\n",
    "    summary.append(f\"Muestras: {len(df)}\")\n",
    "    \n",
    "    # Análisis de tipos\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        summary.append(f\"  - {dtype}: {count}\")\n",
    "    \n",
    "    # Información de tipos de columnas si está disponible\n",
    "    if column_types:\n",
    "        summary.append(\"\\nDistribución de columnas:\")\n",
    "        summary.append(f\"  - Booleanas: {len(column_types.get('boolean_cols', []))}\")\n",
    "        summary.append(f\"  - Numéricas: {len(column_types.get('numeric_cols', []))}\")\n",
    "    \n",
    "    # Análisis de completitud\n",
    "    completeness = df.notna().mean().mean()\n",
    "    summary.append(f\"\\nCompletitud promedio: {completeness:.2%}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_pdf_report(summaries, output_path):\n",
    "    \"\"\"Genera un reporte PDF con los resúmenes de columnas.\"\"\"\n",
    "    pdf_path = os.path.join(output_path, \"feature_engineering_report.pdf\")\n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Título\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30\n",
    "    )\n",
    "    story.append(Paragraph(\"Reporte de Feature Engineering\", title_style))\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    # Fecha y hora\n",
    "    date_style = ParagraphStyle(\n",
    "        'DateStyle',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=12,\n",
    "        spaceAfter=30\n",
    "    )\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    story.append(Paragraph(f\"Generado el: {current_time}\", date_style))\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    # Contenido principal\n",
    "    for section_title, content in summaries.items():\n",
    "        # Título de sección\n",
    "        section_style = ParagraphStyle(\n",
    "            'SectionTitle',\n",
    "            parent=styles['Heading2'],\n",
    "            fontSize=16,\n",
    "            spaceAfter=12\n",
    "        )\n",
    "        story.append(Paragraph(section_title, section_style))\n",
    "        story.append(Spacer(1, 10))\n",
    "\n",
    "        # Contenido de la sección\n",
    "        content_style = ParagraphStyle(\n",
    "            'ContentStyle',\n",
    "            parent=styles['Normal'],\n",
    "            fontSize=12,\n",
    "            leftIndent=20,\n",
    "            spaceAfter=6\n",
    "        )\n",
    "        \n",
    "        if isinstance(content, list):\n",
    "            for line in content:\n",
    "                story.append(Paragraph(str(line), content_style))\n",
    "        else:\n",
    "            story.append(Paragraph(str(content), content_style))\n",
    "        \n",
    "        story.append(Spacer(1, 20))\n",
    "\n",
    "    # Generar PDF\n",
    "    doc.build(story)\n",
    "    return pdf_path\n",
    "\n",
    "def main():\n",
    "    # Cargar configuración\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    config_path = os.path.join(base_path, \"config.json\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    output_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"featured\"])\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Inicializar diccionario de resúmenes\n",
    "    summaries = {}\n",
    "    \n",
    "    # Cargar datos\n",
    "    encoded_path = os.path.join(base_path, config[\"paths\"][\"intermediate\"][\"encoded\"], \"df_triage_encoded.parquet\")\n",
    "    df = pd.read_parquet(encoded_path)\n",
    "    \n",
    "    # Resumen inicial\n",
    "    summaries[\"Estado Inicial\"] = get_column_summary(\"Inicial\", df)\n",
    "    initial_columns = len(df.columns)\n",
    "    \n",
    "    # 1. Identificar columnas para procesar\n",
    "    print(\"Identificando columnas...\")\n",
    "    column_types = identify_columns_for_processing(df)\n",
    "    numeric_cols = column_types.get('numeric_cols', [])\n",
    "    \n",
    "    summaries[\"Clasificación de Columnas\"] = get_column_summary(\"Post-clasificación\", df, column_types)\n",
    "    \n",
    "    # 2. Imputación multivariada solo en columnas numéricas no booleanas\n",
    "    print(\"Realizando imputación...\")\n",
    "    imputer = IterativeImputer(max_iter=15, random_state=42)\n",
    "    df_numeric_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df[numeric_cols]),\n",
    "        columns=numeric_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # 3. Generación de características en lotes\n",
    "    print(\"Generando características...\")\n",
    "    batch_size = 50  # Procesar 50 columnas a la vez\n",
    "    new_features_dfs = []\n",
    "    \n",
    "    for i in range(0, len(numeric_cols), batch_size):\n",
    "        batch_cols = numeric_cols[i:i + batch_size]\n",
    "        batch_features = batch_safe_transformation(df_numeric_imputed, batch_cols)\n",
    "        new_features_dfs.append(batch_features)\n",
    "    \n",
    "    new_features = pd.concat(new_features_dfs, axis=1)\n",
    "    \n",
    "    summaries[\"Generación de Features\"] = [\n",
    "        f\"Features generadas: {len(new_features.columns)}\",\n",
    "        \"Tipos de transformaciones:\",\n",
    "        \"  - Logarítmicas (log1p)\",\n",
    "        \"  - Raíz cuadrada\",\n",
    "        \"  - Cuadráticas\",\n",
    "        \"  - Z-score\"\n",
    "    ]\n",
    "    \n",
    "    # 4. Interacciones seguras\n",
    "    print(\"Generando interacciones...\")\n",
    "    safe_interactions = generate_safe_interactions(\n",
    "        pd.concat([df_numeric_imputed, df[['nivel_triage']]], axis=1),\n",
    "        numeric_cols\n",
    "    )\n",
    "    \n",
    "    summaries[\"Interacciones\"] = [\n",
    "        f\"Interacciones generadas: {len(safe_interactions.columns)}\",\n",
    "        \"Método: Target Encoding con interacciones\"\n",
    "    ]\n",
    "    \n",
    "    # 5. Selección de características\n",
    "    print(\"Seleccionando características...\")\n",
    "    full_set = pd.concat([\n",
    "        df_numeric_imputed,\n",
    "        new_features,\n",
    "        safe_interactions,\n",
    "        df[column_types.get('boolean_cols', [])],  # Mantener columnas booleanas sin procesar\n",
    "    ], axis=1)\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    selected = selector.fit_transform(full_set)\n",
    "    selected_cols = full_set.columns[selector.get_support()]\n",
    "    \n",
    "    # 6. Dataset final\n",
    "    df_feateng = pd.concat([\n",
    "        df[['nivel_triage']],\n",
    "        pd.DataFrame(selected, columns=selected_cols, index=df.index)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Resumen final\n",
    "    summaries[\"Resumen Final\"] = get_column_summary(\"Final\", df_feateng)\n",
    "    \n",
    "    summaries[\"Resumen Comparativo\"] = [\n",
    "        f\"Columnas iniciales: {initial_columns}\",\n",
    "        f\"Features generadas: {len(new_features.columns)}\",\n",
    "        f\"Interacciones generadas: {len(safe_interactions.columns)}\",\n",
    "        f\"Columnas finales: {len(df_feateng.columns)}\"\n",
    "    ]\n",
    "    \n",
    "    # Generar PDF\n",
    "    print(\"Generando reporte PDF...\")\n",
    "    pdf_path = generate_pdf_report(summaries, output_path)\n",
    "    \n",
    "    # Guardar dataset\n",
    "    output_file = os.path.join(output_path, \"df_feateng.parquet\")\n",
    "    pq.write_table(pa.Table.from_pandas(df_feateng), output_file)\n",
    "    \n",
    "    print(f\"\\n✅ Feature engineering completado.\")\n",
    "    print(f\"Dataset guardado en: {output_file}\")\n",
    "    print(f\"Reporte PDF generado en: {pdf_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivos en: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\n",
      "Subdirectorios: ['MaxAbs', 'MinMax', 'NoNorm', 'Robust', 'Standard']\n",
      "Archivos: []\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\\MaxAbs\n",
      "Subdirectorios: []\n",
      "Archivos: ['MaxAbs_all_features.parquet', 'MaxAbs_Linear_plot.png', 'MaxAbs_Linear_report.csv', 'MaxAbs_Linear_selected.parquet', 'MaxAbs_Model-Based_plot.png', 'MaxAbs_Model-Based_report.csv', 'MaxAbs_Model-Based_selected.parquet', 'MaxAbs_Nonlinear_plot.png', 'MaxAbs_Nonlinear_report.csv', 'MaxAbs_Nonlinear_selected.parquet']\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\\MinMax\n",
      "Subdirectorios: []\n",
      "Archivos: ['MinMax_all_features.parquet', 'MinMax_Linear_plot.png', 'MinMax_Linear_report.csv', 'MinMax_Linear_selected.parquet', 'MinMax_Model-Based_plot.png', 'MinMax_Model-Based_report.csv', 'MinMax_Model-Based_selected.parquet', 'MinMax_Nonlinear_plot.png', 'MinMax_Nonlinear_report.csv', 'MinMax_Nonlinear_selected.parquet']\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\\NoNorm\n",
      "Subdirectorios: []\n",
      "Archivos: ['NoNorm_all_features.parquet', 'NoNorm_Linear_plot.png', 'NoNorm_Linear_report.csv', 'NoNorm_Linear_selected.parquet', 'NoNorm_Model-Based_plot.png', 'NoNorm_Model-Based_report.csv', 'NoNorm_Model-Based_selected.parquet', 'NoNorm_Nonlinear_plot.png', 'NoNorm_Nonlinear_report.csv', 'NoNorm_Nonlinear_selected.parquet']\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\\Robust\n",
      "Subdirectorios: []\n",
      "Archivos: ['Robust_all_features.parquet', 'Robust_Linear_plot.png', 'Robust_Linear_report.csv', 'Robust_Linear_selected.parquet', 'Robust_Model-Based_plot.png', 'Robust_Model-Based_report.csv', 'Robust_Model-Based_selected.parquet', 'Robust_Nonlinear_plot.png', 'Robust_Nonlinear_report.csv', 'Robust_Nonlinear_selected.parquet']\n",
      "Directorio actual: C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\\Standard\n",
      "Subdirectorios: []\n",
      "Archivos: ['Standard_all_features.parquet', 'Standard_Linear_plot.png', 'Standard_Linear_report.csv', 'Standard_Linear_selected.parquet', 'Standard_Model-Based_plot.png', 'Standard_Model-Based_report.csv', 'Standard_Model-Based_selected.parquet', 'Standard_Nonlinear_plot.png', 'Standard_Nonlinear_report.csv', 'Standard_Nonlinear_selected.parquet']\n",
      "\n",
      "üîç Encontrados 20 archivos Parquet:\n",
      "‚Ä¢ MaxAbs\\MaxAbs_all_features.parquet\n",
      "‚Ä¢ MaxAbs\\MaxAbs_Linear_selected.parquet\n",
      "‚Ä¢ MaxAbs\\MaxAbs_Model-Based_selected.parquet\n",
      "‚Ä¢ MaxAbs\\MaxAbs_Nonlinear_selected.parquet\n",
      "‚Ä¢ MinMax\\MinMax_all_features.parquet\n",
      "‚Ä¢ MinMax\\MinMax_Linear_selected.parquet\n",
      "‚Ä¢ MinMax\\MinMax_Model-Based_selected.parquet\n",
      "‚Ä¢ MinMax\\MinMax_Nonlinear_selected.parquet\n",
      "‚Ä¢ NoNorm\\NoNorm_all_features.parquet\n",
      "‚Ä¢ NoNorm\\NoNorm_Linear_selected.parquet\n",
      "‚Ä¢ NoNorm\\NoNorm_Model-Based_selected.parquet\n",
      "‚Ä¢ NoNorm\\NoNorm_Nonlinear_selected.parquet\n",
      "‚Ä¢ Robust\\Robust_all_features.parquet\n",
      "‚Ä¢ Robust\\Robust_Linear_selected.parquet\n",
      "‚Ä¢ Robust\\Robust_Model-Based_selected.parquet\n",
      "‚Ä¢ Robust\\Robust_Nonlinear_selected.parquet\n",
      "‚Ä¢ Standard\\Standard_all_features.parquet\n",
      "‚Ä¢ Standard\\Standard_Linear_selected.parquet\n",
      "‚Ä¢ Standard\\Standard_Model-Based_selected.parquet\n",
      "‚Ä¢ Standard\\Standard_Nonlinear_selected.parquet\n",
      "\n",
      "[1/20] Procesando: MaxAbs_all_features.parquet\n",
      "‚úÖ MaxAbs_all_features.parquet procesado correctamente\n",
      "\n",
      "[2/20] Procesando: MaxAbs_Linear_selected.parquet\n",
      "‚úÖ MaxAbs_Linear_selected.parquet procesado correctamente\n",
      "\n",
      "[3/20] Procesando: MaxAbs_Model-Based_selected.parquet\n",
      "‚úÖ MaxAbs_Model-Based_selected.parquet procesado correctamente\n",
      "\n",
      "[4/20] Procesando: MaxAbs_Nonlinear_selected.parquet\n",
      "‚úÖ MaxAbs_Nonlinear_selected.parquet procesado correctamente\n",
      "\n",
      "[5/20] Procesando: MinMax_all_features.parquet\n",
      "‚úÖ MinMax_all_features.parquet procesado correctamente\n",
      "\n",
      "[6/20] Procesando: MinMax_Linear_selected.parquet\n",
      "‚úÖ MinMax_Linear_selected.parquet procesado correctamente\n",
      "\n",
      "[7/20] Procesando: MinMax_Model-Based_selected.parquet\n",
      "‚úÖ MinMax_Model-Based_selected.parquet procesado correctamente\n",
      "\n",
      "[8/20] Procesando: MinMax_Nonlinear_selected.parquet\n",
      "‚úÖ MinMax_Nonlinear_selected.parquet procesado correctamente\n",
      "\n",
      "[9/20] Procesando: NoNorm_all_features.parquet\n",
      "‚úÖ NoNorm_all_features.parquet procesado correctamente\n",
      "\n",
      "[10/20] Procesando: NoNorm_Linear_selected.parquet\n",
      "‚úÖ NoNorm_Linear_selected.parquet procesado correctamente\n",
      "\n",
      "[11/20] Procesando: NoNorm_Model-Based_selected.parquet\n",
      "‚úÖ NoNorm_Model-Based_selected.parquet procesado correctamente\n",
      "\n",
      "[12/20] Procesando: NoNorm_Nonlinear_selected.parquet\n",
      "‚úÖ NoNorm_Nonlinear_selected.parquet procesado correctamente\n",
      "\n",
      "[13/20] Procesando: Robust_all_features.parquet\n",
      "‚ùå Error en Robust_all_features.parquet: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device\n",
      "\n",
      "[14/20] Procesando: Robust_Linear_selected.parquet\n",
      "‚úÖ Robust_Linear_selected.parquet procesado correctamente\n",
      "\n",
      "[15/20] Procesando: Robust_Model-Based_selected.parquet\n",
      "‚úÖ Robust_Model-Based_selected.parquet procesado correctamente\n",
      "\n",
      "[16/20] Procesando: Robust_Nonlinear_selected.parquet\n",
      "‚úÖ Robust_Nonlinear_selected.parquet procesado correctamente\n",
      "\n",
      "[17/20] Procesando: Standard_all_features.parquet\n",
      "‚ùå Error en Standard_all_features.parquet: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device\n",
      "\n",
      "[18/20] Procesando: Standard_Linear_selected.parquet\n",
      "‚úÖ Standard_Linear_selected.parquet procesado correctamente\n",
      "\n",
      "[19/20] Procesando: Standard_Model-Based_selected.parquet\n",
      "‚úÖ Standard_Model-Based_selected.parquet procesado correctamente\n",
      "\n",
      "[20/20] Procesando: Standard_Nonlinear_selected.parquet\n",
      "‚úÖ Standard_Nonlinear_selected.parquet procesado correctamente\n",
      "\n",
      "üéâ Proceso completado: 18 √©xitos, 2 fallos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configuraci√≥n\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MIN_SAMPLES_PER_CLASS = 25000  # Ajusta este valor seg√∫n sea necesario\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename='data_split.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def extract_metadata_from_filename(file_path):\n",
    "    \"\"\"\n",
    "    Extrae metadatos del nombre del archivo y su ruta para mayor robustez.\n",
    "    \"\"\"\n",
    "    # Obtener el nombre del archivo y la ruta completa\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Patr√≥n para archivos que terminan en _all_features.parquet\n",
    "    all_features_pattern = r\"\"\"\n",
    "        ^(?P<normalization>[A-Za-z]+)_  # Parte de normalizaci√≥n\n",
    "        all_features\\.parquet$          # Tipo de dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Patr√≥n para archivos que terminan en _selected.parquet\n",
    "    selected_pattern = r\"\"\"\n",
    "        ^(?P<normalization>[A-Za-z]+)_  # Parte de normalizaci√≥n\n",
    "        (?P<selector>[A-Za-z-]+)_       # Selector de caracter√≠sticas\n",
    "        selected\\.parquet$              # Tipo de dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Intentar coincidir con el patr√≥n de \"all_features\"\n",
    "    all_features_match = re.match(all_features_pattern, file_name, re.VERBOSE)\n",
    "    if all_features_match:\n",
    "        normalization = all_features_match.group(\"normalization\")\n",
    "        selector = \"full\"  # Asignar \"full\" como selector para estos archivos\n",
    "        file_type = \"full\"\n",
    "        return normalization, selector, file_type\n",
    "\n",
    "    # Intentar coincidir con el patr√≥n de \"selected\"\n",
    "    selected_match = re.match(selected_pattern, file_name, re.VERBOSE)\n",
    "    if selected_match:\n",
    "        normalization = selected_match.group(\"normalization\")\n",
    "        selector = selected_match.group(\"selector\")\n",
    "        file_type = \"selected\"\n",
    "        return normalization, selector, file_type\n",
    "\n",
    "    # Si no coincide con ning√∫n patr√≥n, devolver None\n",
    "    return None, None, None\n",
    "\n",
    "def validate_data_structure(df):\n",
    "    \"\"\"Realiza validaciones completas de la estructura del DataFrame\"\"\"\n",
    "    # Verificar columna target\n",
    "    if 'nivel_triage' not in df.columns:\n",
    "        raise ValueError(\"Columna target 'nivel_triage' no encontrada\")\n",
    "    \n",
    "    # Verificar valores del target\n",
    "    target_values = df['nivel_triage'].unique()\n",
    "    if any(not isinstance(x, (int, np.integer)) for x in target_values):\n",
    "        raise ValueError(\"Valores no enteros en la columna target\")\n",
    "    \n",
    "    # Verificar distribuci√≥n de clases\n",
    "    class_distribution = df['nivel_triage'].value_counts()\n",
    "    logging.info(f\"Distribuci√≥n de clases en el dataset: {class_distribution.to_dict()}\")\n",
    "    \n",
    "    # Ajustar el umbral m√≠nimo de muestras por clase\n",
    "    if class_distribution.min() < MIN_SAMPLES_PER_CLASS:\n",
    "        problematic = class_distribution[class_distribution < MIN_SAMPLES_PER_CLASS]\n",
    "        raise ValueError(\n",
    "            f\"Clases con menos de {MIN_SAMPLES_PER_CLASS} muestras: {problematic.to_dict()}\"\n",
    "        )\n",
    "    \n",
    "    return True\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"Balancea las clases usando SMOTE\"\"\"\n",
    "    smote = SMOTE(sampling_strategy='auto', k_neighbors=3, random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "def safe_stratified_split(X, y):\n",
    "    \"\"\"Realiza split estratificado con manejo de clases peque√±as\"\"\"\n",
    "    class_counts = y.value_counts()\n",
    "    \n",
    "    # Verificar si podemos estratificar\n",
    "    can_stratify = all(count >= (1 / TEST_SIZE) for count in class_counts)\n",
    "    \n",
    "    if can_stratify:\n",
    "        # Realizar split estratificado\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y\n",
    "        )\n",
    "    else:\n",
    "        logging.warning(\"Estratificaci√≥n deshabilitada por clases muy peque√±as. Ajustando tama√±o de prueba...\")\n",
    "        # Ajustar el tama√±o de prueba para asegurar que todas las clases est√©n representadas\n",
    "        min_samples = class_counts.min()\n",
    "        test_size = min(1 / min_samples, TEST_SIZE)  # Asegurar al menos una muestra por clase en el test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=test_size,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y\n",
    "        )\n",
    "    \n",
    "    # Verificar que todas las clases est√©n presentes en ambos conjuntos\n",
    "    train_classes = set(y_train.unique())\n",
    "    test_classes = set(y_test.unique())\n",
    "    \n",
    "    if train_classes != test_classes:\n",
    "        missing_train = test_classes - train_classes\n",
    "        missing_test = train_classes - test_classes\n",
    "        logging.warning(f\"Clases faltantes en train: {missing_train}. Clases faltantes en test: {missing_test}. Ajustando split...\")\n",
    "        \n",
    "        # Forzar presencia de todas las clases en train y test\n",
    "        for cls in missing_train:\n",
    "            idx = y_test[y_test == cls].index\n",
    "            X_train = pd.concat([X_train, X_test.loc[idx]])\n",
    "            y_train = pd.concat([y_train, y_test.loc[idx]])\n",
    "            X_test = X_test.drop(idx)\n",
    "            y_test = y_test.drop(idx)\n",
    "        \n",
    "        for cls in missing_test:\n",
    "            idx = y_train[y_train == cls].index\n",
    "            X_test = pd.concat([X_test, X_train.loc[idx]])\n",
    "            y_test = pd.concat([y_test, y_train.loc[idx]])\n",
    "            X_train = X_train.drop(idx)\n",
    "            y_train = y_train.drop(idx)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def optimize_dtypes(df, target_column):\n",
    "    \"\"\"Optimiza los tipos de datos del DataFrame\"\"\"\n",
    "    for col in df.columns:\n",
    "        if col == target_column:\n",
    "            if df[col].dtype != 'int8':\n",
    "                df[col] = df[col].astype('int8')\n",
    "        else:\n",
    "            if df[col].dtype != 'float32':\n",
    "                # Conservar solo las columnas con varianza\n",
    "                if df[col].nunique() > 1:\n",
    "                    df[col] = df[col].astype('float32')\n",
    "                else:\n",
    "                    df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_train_test_split(input_file, output_base_path):\n",
    "    \"\"\"Crea splits train-test con validaciones mejoradas\"\"\"\n",
    "    try:\n",
    "        file_name = os.path.basename(input_file)\n",
    "        logging.info(f\"Iniciando procesamiento de: {file_name}\")\n",
    "        \n",
    "        # Extraer metadatos usando la ruta completa\n",
    "        norm_name, selector_name, file_type = extract_metadata_from_filename(input_file)\n",
    "        if not all([norm_name, selector_name, file_type]):\n",
    "            raise ValueError(f\"Formato de nombre inv√°lido: {file_name}\")\n",
    "        \n",
    "        # Leer dataset\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Archivo no encontrado: {input_file}\")\n",
    "        \n",
    "        df = pd.read_parquet(input_file)\n",
    "        \n",
    "        # Verificar el n√∫mero de filas cargadas\n",
    "        logging.info(f\"N√∫mero de filas cargadas: {len(df)}\")\n",
    "        \n",
    "        # Validar estructura\n",
    "        validate_data_structure(df)\n",
    "        \n",
    "        # Optimizar tipos de datos\n",
    "        df = optimize_dtypes(df, 'nivel_triage')\n",
    "        \n",
    "        # Preparar split\n",
    "        X = df.drop(columns=['nivel_triage'])\n",
    "        y = df['nivel_triage']\n",
    "        \n",
    "        # Balancear clases si es necesario\n",
    "        X_res, y_res = balance_classes(X, y)\n",
    "        \n",
    "        # Split estratificado seguro\n",
    "        X_train, X_test, y_train, y_test = safe_stratified_split(X_res, y_res)\n",
    "        \n",
    "        # Validar distribuci√≥n post-split\n",
    "        train_classes = set(y_train.unique())\n",
    "        test_classes = set(y_test.unique())\n",
    "        \n",
    "        if train_classes != test_classes:\n",
    "            missing = test_classes - train_classes\n",
    "            logging.warning(f\"Clases faltantes en train: {missing}. Ajustando split...\")\n",
    "            # Forzar presencia de todas las clases en train\n",
    "            _, X_test, _, y_test = train_test_split(\n",
    "                X_test, y_test,\n",
    "                test_size=TEST_SIZE,\n",
    "                random_state=RANDOM_STATE,\n",
    "                stratify=y_test if len(y_test) > 0 else None\n",
    "            )\n",
    "        \n",
    "        # Crear estructura de directorios\n",
    "        file_id = f\"{norm_name}_{selector_name}_{file_type}\"\n",
    "        output_path = os.path.join(output_base_path, file_id)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        # Guardar archivos con compresi√≥n\n",
    "        save_params = {\n",
    "            'engine': 'pyarrow',\n",
    "            'compression': 'gzip',\n",
    "            'index': False\n",
    "        }\n",
    "        \n",
    "        X_train.to_parquet(os.path.join(output_path, f\"X_train_{file_id}.parquet\"), **save_params)\n",
    "        X_test.to_parquet(os.path.join(output_path, f\"X_test_{file_id}.parquet\"), **save_params)\n",
    "        y_train.to_frame().to_parquet(os.path.join(output_path, f\"y_train_{file_id}.parquet\"), **save_params)\n",
    "        y_test.to_frame().to_parquet(os.path.join(output_path, f\"y_test_{file_id}.parquet\"), **save_params)\n",
    "        \n",
    "        # Loggear m√©tricas\n",
    "        split_info = {\n",
    "            'original_samples': len(df),\n",
    "            'train_samples': len(X_train),\n",
    "            'test_samples': len(X_test),\n",
    "            'train_classes': y_train.nunique(),\n",
    "            'test_classes': y_test.nunique(),\n",
    "            'class_distribution_train': y_train.value_counts(normalize=True).to_dict(),\n",
    "            'class_distribution_test': y_test.value_counts(normalize=True).to_dict()\n",
    "        }\n",
    "        logging.info(f\"Split exitoso: {split_info}\")\n",
    "        \n",
    "        print(f\"‚úÖ {file_name} procesado correctamente\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error procesando {file_name}: {str(e)}\", exc_info=True)\n",
    "        print(f\"‚ùå Error en {file_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    input_root = r\"C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\\experimento_final_20250223_181000\"\n",
    "    output_root = os.path.join(r\"C:\\Users\\Administrador\\Documents\\PythonScripts\\Tesis\\tesisaustral\\outputs\\experiments\", f\"train_test_splits_{timestamp}\")\n",
    "    \n",
    "    # Verificar la ruta de entrada\n",
    "    print(f\"Buscando archivos en: {input_root}\")\n",
    "    if not os.path.exists(input_root):\n",
    "        raise FileNotFoundError(f\"La ruta de entrada no existe: {input_root}\")\n",
    "    \n",
    "    # Buscar recursivamente TODOS los archivos .parquet\n",
    "    target_files = []\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        print(f\"Directorio actual: {root}\")\n",
    "        print(f\"Subdirectorios: {dirs}\")\n",
    "        print(f\"Archivos: {files}\")\n",
    "        for file in files:\n",
    "            if file.endswith(\".parquet\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                target_files.append(full_path)\n",
    "    \n",
    "    print(f\"\\nüîç Encontrados {len(target_files)} archivos Parquet:\")\n",
    "    for f in target_files:\n",
    "        print(f\"‚Ä¢ {os.path.relpath(f, input_root)}\")\n",
    "    \n",
    "    # Procesar con manejo de errores\n",
    "    success_count = 0\n",
    "    for idx, file_path in enumerate(target_files, 1):\n",
    "        print(f\"\\n[{idx}/{len(target_files)}] Procesando: {os.path.basename(file_path)}\")\n",
    "        if create_train_test_split(file_path, output_root):\n",
    "            success_count += 1\n",
    "    \n",
    "    # Reporte final\n",
    "    print(f\"\\nüéâ Proceso completado: {success_count} √©xitos, {len(target_files)-success_count} fallos\")\n",
    "    logging.info(f\"Proceso finalizado: {success_count}/{len(target_files)} exitosos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
